#!/usr/bin/env python3
"""
–ü–û–õ–ù–û–ï –ü–†–û–î–£–ö–¶–ò–û–ù–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï –ê–ù–°–ê–ú–ë–õ–Ø ML-–ú–û–î–ï–õ–ï–ô
=====================================================

–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –í–°–ï–• –∏–º–µ—é—â–∏—Ö—Å—è –¥–∞–Ω–Ω—ã—Ö –∑–∞ 90 –¥–Ω–µ–π –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π:
- –í—Å–µ 168K+ –∑–∞–ø–∏—Å–µ–π –∏–∑ 8 —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤
- –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–µ–π –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
- –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
- –î–µ—Ç–∞–ª—å–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ –∏ –º–µ—Ç—Ä–∏–∫–∏
- –ü—Ä–æ–¥—É–∫—Ü–∏–æ–Ω–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π

–í–†–ï–ú–Ø –í–´–ü–û–õ–ù–ï–ù–ò–Ø: ~15-30 –º–∏–Ω—É—Ç
–¢–†–ï–ë–û–í–ê–ù–ò–Ø –û–ó–£: ~2-4 –ì–ë
"""

import sys
import warnings
import time
from pathlib import Path
import pandas as pd
import numpy as np
from datetime import datetime
import joblib

# ML –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    classification_report, confusion_matrix, accuracy_score,
    precision_score, recall_score, f1_score, roc_auc_score,
    precision_recall_curve, roc_curve
)
from sklearn.linear_model import LogisticRegression

# –ë–∞–∑–æ–≤—ã–µ ML –º–æ–¥–µ–ª–∏
from sklearn.ensemble import RandomForestClassifier
try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
    print("‚ö†Ô∏è LightGBM not available")

import xgboost as xgb
import catboost as cb

# –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
import ta
from ta.momentum import RSIIndicator, StochasticOscillator, WilliamsRIndicator
from ta.trend import MACD, EMAIndicator, SMAIndicator, ADXIndicator
from ta.volatility import BollingerBands, AverageTrueRange
from ta.volume import OnBalanceVolumeIndicator

warnings.filterwarnings('ignore')


class ProductionEnsembleTrainer:
    """–ü—Ä–æ–¥—É–∫—Ü–∏–æ–Ω–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä –∞–Ω—Å–∞–º–±–ª—è —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –∫–∞—á–µ—Å—Ç–≤–æ–º."""
    
    def __init__(self, output_dir: str = "models/ensemble_live"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–´–ï –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—Ä–æ–¥—É–∫—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
        print("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–¥—É–∫—Ü–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π...")
        
        # Random Forest - —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.base_models = {
            'random_forest': RandomForestClassifier(
                n_estimators=500,      # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 100
                max_depth=15,          # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 8
                min_samples_split=5,   # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –±–æ–ª—å—à–µ–π –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏
                min_samples_leaf=2,    # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –±–æ–ª—å—à–µ–π –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏
                max_features='sqrt',
                bootstrap=True,
                oob_score=True,        # Out-of-bag scoring
                random_state=42,
                n_jobs=-1,
                verbose=1
            )
        }
        
        # XGBoost - –ø—Ä–æ–¥—É–∫—Ü–∏–æ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.base_models['xgboost'] = xgb.XGBClassifier(
            n_estimators=500,
            max_depth=8,
            learning_rate=0.05,      # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            subsample=0.8,
            colsample_bytree=0.8,
            reg_alpha=0.1,
            reg_lambda=0.1,
            scale_pos_weight=3,      # –î–ª—è –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤
            random_state=42,
            n_jobs=-1,
            eval_metric='logloss',
            verbosity=1
        )
        
        # CatBoost - –ø—Ä–æ–¥—É–∫—Ü–∏–æ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.base_models['catboost'] = cb.CatBoostClassifier(
            iterations=500,
            depth=8,
            learning_rate=0.05,
            l2_leaf_reg=3,
            border_count=128,
            feature_border_type='GreedyLogSum',
            bootstrap_type='Bayesian',
            bagging_temperature=1,
            random_state=42,
            verbose=100,  # –ö–∞–∂–¥—ã–µ 100 –∏—Ç–µ—Ä–∞—Ü–∏–π
            early_stopping_rounds=50
        )
        
        # LightGBM –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        if LIGHTGBM_AVAILABLE:
            self.base_models['lightgbm'] = lgb.LGBMClassifier(
                n_estimators=500,
                max_depth=8,
                learning_rate=0.05,
                num_leaves=31,
                subsample=0.8,
                colsample_bytree=0.8,
                reg_alpha=0.1,
                reg_lambda=0.1,
                scale_pos_weight=3,
                random_state=42,
                n_jobs=-1,
                verbose=100,
                early_stopping_rounds=50
            )
        
        # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –º–µ—Ç–∞–º–æ–¥–µ–ª—å
        self.meta_model = LogisticRegression(
            C=0.1,                    # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
            penalty='elasticnet',     # Elastic Net —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
            l1_ratio=0.5,            # –ë–∞–ª–∞–Ω—Å L1/L2
            solver='saga',           # Solver –¥–ª—è elastic net
            max_iter=5000,           # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏
            random_state=42,
            n_jobs=-1
        )
        
        # –°–∫–∞–ª–µ—Ä
        self.scaler = StandardScaler()
        self.feature_names = []
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self.training_stats = {}
        self.feature_importance = {}
        
        print(f"‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã {len(self.base_models)} –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–∏")
        print(f"üìä –û–∂–∏–¥–∞–µ–º–æ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: 15-30 –º–∏–Ω—É—Ç")
    
    def load_full_dataset(self) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –í–°–ï–• –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π."""
        print("\n" + "="*60)
        print("üìä –ó–ê–ì–†–£–ó–ö–ê –ü–û–õ–ù–û–ì–û –î–ê–¢–ê–°–ï–¢–ê (–ë–ï–ó –û–ì–†–ê–ù–ò–ß–ï–ù–ò–ô)")
        print("="*60)
        
        data_dir = Path("data/bybit_futures")
        combined_data = []
        total_original = 0
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –í–°–ï —Ñ–∞–π–ª—ã –ø–æ–ª–Ω–æ—Å—Ç—å—é
        for file_path in sorted(data_dir.glob("*90d_bybit_futures.csv")):
            timeframe = file_path.name.split("_")[1]
            
            try:
                start_time = time.time()
                df = pd.read_csv(file_path)
                df['timestamp'] = pd.to_datetime(df['timestamp'])
                df['timeframe'] = timeframe
                
                combined_data.append(df)
                total_original += len(df)
                
                load_time = time.time() - start_time
                print(f"   ‚úÖ {timeframe:>4s}: {len(df):>7,} –∑–∞–ø–∏—Å–µ–π ({load_time:.1f}s)")
                
            except Exception as e:
                print(f"   ‚ùå {timeframe}: –û—à–∏–±–∫–∞ - {e}")
                continue
        
        if not combined_data:
            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
        print("\nüîÑ –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö...")
        start_time = time.time()
        
        all_data = pd.concat(combined_data, ignore_index=True)
        all_data = all_data.sort_values(['timeframe', 'timestamp']).reset_index(drop=True)
        
        combine_time = time.time() - start_time
        
        print(f"‚úÖ –ü–û–õ–ù–´–ô –î–ê–¢–ê–°–ï–¢ –ó–ê–ì–†–£–ñ–ï–ù:")
        print(f"   üìä –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(all_data):,}")
        print(f"   üìÖ –ü–µ—Ä–∏–æ–¥: {all_data['timestamp'].min()} ‚Üí {all_data['timestamp'].max()}")
        print(f"   ‚è∞ –¢–∞–π–º—Ñ—Ä–µ–π–º—ã: {sorted(all_data['timeframe'].unique())}")
        print(f"   ‚è±Ô∏è  –í—Ä–µ–º—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è: {combine_time:.1f}s")
        print(f"   üíæ –†–∞–∑–º–µ—Ä –≤ –ø–∞–º—è—Ç–∏: ~{all_data.memory_usage(deep=True).sum() / 1024**2:.0f} –ú–ë")
        
        return all_data
    
    def generate_advanced_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤."""
        print("\n" + "="*60)  
        print("üîß –ì–ï–ù–ï–†–ê–¶–ò–Ø –†–ê–°–®–ò–†–ï–ù–ù–´–• –¢–ï–•–ù–ò–ß–ï–°–ö–ò–• –ü–†–ò–ó–ù–ê–ö–û–í")
        print("="*60)
        
        processed_groups = []
        total_features_generated = 0
        
        for timeframe in sorted(df['timeframe'].unique()):
            tf_data = df[df['timeframe'] == timeframe].copy()
            tf_data = tf_data.sort_values('timestamp').reset_index(drop=True)
            
            print(f"\nüìà –û–±—Ä–∞–±–æ—Ç–∫–∞ {timeframe} ({len(tf_data):,} –∑–∞–ø–∏—Å–µ–π)...")
            start_time = time.time()
            
            # –ë–∞–∑–æ–≤—ã–µ Moving Averages
            tf_data['MA5'] = tf_data['close'].rolling(5).mean()
            tf_data['MA10'] = tf_data['close'].rolling(10).mean()
            tf_data['MA20'] = tf_data['close'].rolling(20).mean()
            tf_data['MA50'] = tf_data['close'].rolling(50).mean()
            tf_data['MA100'] = tf_data['close'].rolling(100).mean()
            
            # Exponential Moving Averages
            tf_data['EMA8'] = EMAIndicator(tf_data['close'], window=8).ema_indicator()
            tf_data['EMA12'] = EMAIndicator(tf_data['close'], window=12).ema_indicator()
            tf_data['EMA26'] = EMAIndicator(tf_data['close'], window=26).ema_indicator()
            tf_data['EMA50'] = EMAIndicator(tf_data['close'], window=50).ema_indicator()
            
            # MA –∫—Ä–æ—Å—Å–æ–≤–µ—Ä—ã
            tf_data['MA5_MA20_cross'] = (tf_data['MA5'] > tf_data['MA20']).astype(int)
            tf_data['EMA12_EMA26_cross'] = (tf_data['EMA12'] > tf_data['EMA26']).astype(int)
            
            # RSI —Å–µ–º–µ–π—Å—Ç–≤–æ
            tf_data['RSI14'] = RSIIndicator(tf_data['close'], window=14).rsi()
            tf_data['RSI21'] = RSIIndicator(tf_data['close'], window=21).rsi()
            tf_data['RSI_overbought'] = (tf_data['RSI14'] > 70).astype(int)
            tf_data['RSI_oversold'] = (tf_data['RSI14'] < 30).astype(int)
            
            # MACD —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π
            macd = MACD(tf_data['close'])
            tf_data['MACD'] = macd.macd()
            tf_data['MACD_signal'] = macd.macd_signal()
            tf_data['MACD_diff'] = tf_data['MACD'] - tf_data['MACD_signal']
            tf_data['MACD_histogram'] = macd.macd_diff()
            tf_data['MACD_bullish'] = (tf_data['MACD_diff'] > 0).astype(int)
            
            # Bollinger Bands —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π
            bb = BollingerBands(tf_data['close'], window=20)
            tf_data['BB_hband'] = bb.bollinger_hband()
            tf_data['BB_lband'] = bb.bollinger_lband()
            tf_data['BB_width'] = bb.bollinger_wband()
            tf_data['BB_position'] = (tf_data['close'] - tf_data['BB_lband']) / (tf_data['BB_hband'] - tf_data['BB_lband'])
            tf_data['BB_squeeze'] = (tf_data['BB_width'] < tf_data['BB_width'].rolling(20).mean() * 0.8).astype(int)
            
            # Stochastic Oscillator
            stoch = StochasticOscillator(tf_data['high'], tf_data['low'], tf_data['close'])
            tf_data['Stoch_K'] = stoch.stoch()
            tf_data['Stoch_D'] = stoch.stoch_signal()
            tf_data['Stoch_overbought'] = (tf_data['Stoch_K'] > 80).astype(int)
            tf_data['Stoch_oversold'] = (tf_data['Stoch_K'] < 20).astype(int)
            
            # Williams %R
            tf_data['Williams_R'] = WilliamsRIndicator(tf_data['high'], tf_data['low'], tf_data['close']).williams_r()
            
            # ADX (Average Directional Index)
            try:
                adx = ADXIndicator(tf_data['high'], tf_data['low'], tf_data['close'])
                tf_data['ADX'] = adx.adx()
                tf_data['ADX_strong_trend'] = (tf_data['ADX'] > 25).astype(int)
            except:
                tf_data['ADX'] = 0
                tf_data['ADX_strong_trend'] = 0
            
            # Average True Range (–≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å)
            tf_data['ATR'] = AverageTrueRange(tf_data['high'], tf_data['low'], tf_data['close']).average_true_range()
            tf_data['ATR_normalized'] = tf_data['ATR'] / tf_data['close']
            
            # Volume –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            tf_data['volume_sma5'] = tf_data['volume'].rolling(5).mean()
            tf_data['volume_sma20'] = tf_data['volume'].rolling(20).mean()
            tf_data['volume_ratio'] = tf_data['volume'] / tf_data['volume_sma20']
            tf_data['vol_change'] = tf_data['volume'].pct_change()
            tf_data['volume_spike'] = (tf_data['volume'] > tf_data['volume_sma20'] * 2).astype(int)
            
            # On Balance Volume
            try:
                tf_data['OBV'] = OnBalanceVolumeIndicator(tf_data['close'], tf_data['volume']).on_balance_volume()
                tf_data['OBV_sma'] = tf_data['OBV'].rolling(20).mean()
                tf_data['OBV_divergence'] = (tf_data['OBV'] > tf_data['OBV_sma']).astype(int)
            except:
                tf_data['OBV'] = 0
                tf_data['OBV_sma'] = 0
                tf_data['OBV_divergence'] = 0
            
            # Price action –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            tf_data['price_change'] = tf_data['close'].pct_change()
            tf_data['price_change_5'] = tf_data['close'].pct_change(5)
            tf_data['price_change_10'] = tf_data['close'].pct_change(10)
            tf_data['price_change_20'] = tf_data['close'].pct_change(20)
            
            # Volatility
            tf_data['volatility_5'] = tf_data['close'].pct_change().rolling(5).std()
            tf_data['volatility_20'] = tf_data['close'].pct_change().rolling(20).std()
            tf_data['volatility_50'] = tf_data['close'].pct_change().rolling(50).std()
            
            # High/Low –∞–Ω–∞–ª–∏–∑
            tf_data['high_low_pct'] = (tf_data['high'] - tf_data['low']) / tf_data['close']
            tf_data['close_to_high'] = (tf_data['high'] - tf_data['close']) / tf_data['close']
            tf_data['close_to_low'] = (tf_data['close'] - tf_data['low']) / tf_data['close']
            tf_data['upper_shadow'] = (tf_data['high'] - tf_data[['open', 'close']].max(axis=1)) / tf_data['close']
            tf_data['lower_shadow'] = (tf_data[['open', 'close']].min(axis=1) - tf_data['low']) / tf_data['close']
            
            # Gap –∞–Ω–∞–ª–∏–∑
            tf_data['gap'] = (tf_data['open'] - tf_data['close'].shift(1)) / tf_data['close'].shift(1)
            tf_data['gap_up'] = (tf_data['gap'] > 0.001).astype(int)  # >0.1% gap
            tf_data['gap_down'] = (tf_data['gap'] < -0.001).astype(int)
            
            # Support/Resistance levels
            tf_data['close_above_MA20'] = (tf_data['close'] > tf_data['MA20']).astype(int)
            tf_data['close_above_MA50'] = (tf_data['close'] > tf_data['MA50']).astype(int)
            tf_data['price_momentum'] = tf_data['close'] / tf_data['close'].shift(10) - 1
            
            # Timeframe encoding
            timeframe_mapping = {
                '1m': 1, '5m': 5, '15m': 15, '30m': 30, '60m': 60, '1h': 60,
                '120m': 120, '2h': 120, '240m': 240, '4h': 240,
                '720m': 720, '12h': 720, 'D': 1440, '1d': 1440
            }
            tf_data['timeframe_minutes'] = timeframe_mapping.get(timeframe, 5)
            
            # –£–¥–∞–ª—è–µ–º NaN
            original_len = len(tf_data)
            tf_data = tf_data.dropna()
            dropped = original_len - len(tf_data)
            
            processing_time = time.time() - start_time
            
            if len(tf_data) > 0:
                processed_groups.append(tf_data)
                feature_count = len([col for col in tf_data.columns 
                                   if col not in ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'timeframe']])
                total_features_generated += feature_count
                
                print(f"   ‚úÖ {len(tf_data):>6,} –∑–∞–ø–∏—Å–µ–π | {feature_count:>2d} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ | "
                      f"–£–¥–∞–ª–µ–Ω–æ NaN: {dropped:>4,} | –í—Ä–µ–º—è: {processing_time:.1f}s")
            else:
                print(f"   ‚ö†Ô∏è  –í—Å–µ –∑–∞–ø–∏—Å–∏ —É–¥–∞–ª–µ–Ω—ã –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ NaN")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        print(f"\nüîÑ –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        start_time = time.time()
        
        all_features = pd.concat(processed_groups, ignore_index=True)
        all_features = all_features.sort_values(['timeframe', 'timestamp']).reset_index(drop=True)
        
        combine_time = time.time() - start_time
        
        print(f"‚úÖ –ì–ï–ù–ï–†–ê–¶–ò–Ø –ü–†–ò–ó–ù–ê–ö–û–í –ó–ê–í–ï–†–®–ï–ù–ê:")
        print(f"   üìä –ò—Ç–æ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(all_features):,}")
        print(f"   üî¢ –°—Ä–µ–¥–Ω–µ–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {total_features_generated // len(processed_groups)}")
        print(f"   ‚è±Ô∏è  –í—Ä–µ–º—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è: {combine_time:.1f}s")
        print(f"   üíæ –†–∞–∑–º–µ—Ä –≤ –ø–∞–º—è—Ç–∏: ~{all_features.memory_usage(deep=True).sum() / 1024**2:.0f} –ú–ë")
        
        return all_features
    
    def create_advanced_targets(self, df: pd.DataFrame) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö —Ç–∞—Ä–≥–µ—Ç–æ–≤ —Å —É—á–µ—Ç–æ–º —Å–ø–µ—Ü–∏—Ñ–∏–∫–∏ —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤."""
        print("\n" + "="*60)
        print("üéØ –°–û–ó–î–ê–ù–ò–ï –ü–†–û–î–í–ò–ù–£–¢–´–• –¢–ê–†–ì–ï–¢–û–í")
        print("="*60)
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ)
        timeframe_params = {
            '1m': {'horizon': 30, 'threshold': 0.002},   # 30 –º–∏–Ω—É—Ç, 0.2%
            '5m': {'horizon': 12, 'threshold': 0.005},   # 1 —á–∞—Å, 0.5%
            '15m': {'horizon': 8,  'threshold': 0.008},  # 2 —á–∞—Å–∞, 0.8%
            '60m': {'horizon': 6,  'threshold': 0.012},  # 6 —á–∞—Å–æ–≤, 1.2%
            '120m': {'horizon': 4, 'threshold': 0.015},  # 8 —á–∞—Å–æ–≤, 1.5%
            '240m': {'horizon': 3, 'threshold': 0.020},  # 12 —á–∞—Å–æ–≤, 2.0%
            '720m': {'horizon': 2, 'threshold': 0.030},  # 1 –¥–µ–Ω—å, 3.0%
            'D': {'horizon': 2,    'threshold': 0.050}   # 2 –¥–Ω—è, 5.0%
        }
        
        processed_groups = []
        total_buy_signals = 0
        total_records = 0
        
        for timeframe in sorted(df['timeframe'].unique()):
            tf_data = df[df['timeframe'] == timeframe].copy()
            
            params = timeframe_params.get(timeframe, {'horizon': 5, 'threshold': 0.005})
            horizon = params['horizon']
            threshold = params['threshold']
            
            print(f"üìà {timeframe:>4s}: –≥–æ—Ä–∏–∑–æ–Ω—Ç={horizon:>2d} —Å–≤–µ—á–µ–π, –ø–æ—Ä–æ–≥={threshold:.1%}")
            
            # –°–æ–∑–¥–∞–µ–º —Ç–∞—Ä–≥–µ—Ç—ã
            tf_data['future_return'] = tf_data['close'].shift(-horizon) / tf_data['close'] - 1
            tf_data['target'] = (tf_data['future_return'] > threshold).astype(int)
            
            # –£–¥–∞–ª—è–µ–º –∑–∞–ø–∏—Å–∏ –±–µ–∑ —Ç–∞—Ä–≥–µ—Ç–æ–≤
            tf_data = tf_data[tf_data['future_return'].notna()].copy()
            
            if len(tf_data) > 0:
                processed_groups.append(tf_data)
                
                target_dist = tf_data['target'].value_counts().sort_index()
                buy_count = target_dist.get(1, 0)
                buy_pct = buy_count / len(tf_data) * 100
                
                total_buy_signals += buy_count
                total_records += len(tf_data)
                
                print(f"        {len(tf_data):>7,} –∑–∞–ø–∏—Å–µ–π | BUY: {buy_count:>5,} ({buy_pct:>5.1f}%)")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
        all_targets = pd.concat(processed_groups, ignore_index=True)
        
        final_dist = all_targets['target'].value_counts().sort_index()
        print(f"\nüìä –ò–¢–û–ì–û–í–û–ï –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –¢–ê–†–ì–ï–¢–û–í:")
        print(f"   0 (HOLD/SELL): {final_dist.get(0, 0):>7,} ({final_dist.get(0, 0)/len(all_targets)*100:>5.1f}%)")
        print(f"   1 (BUY):       {final_dist.get(1, 0):>7,} ({final_dist.get(1, 0)/len(all_targets)*100:>5.1f}%)")
        print(f"   üìä –û–±—â–∏–π –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: {final_dist.get(1, 0) / final_dist.get(0, 1):.3f}")
        
        return all_targets
    
    def train_production_models(self, X_train, X_test, y_train, y_test):
        """–ü—Ä–æ–¥—É–∫—Ü–∏–æ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º."""
        print("\n" + "="*60)
        print("ü§ñ –ü–†–û–î–£–ö–¶–ò–û–ù–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï –ë–ê–ó–û–í–´–• –ú–û–î–ï–õ–ï–ô")
        print("="*60)
        
        trained_models = {}
        base_predictions = {}
        model_scores = {}
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        
        for name, model in self.base_models.items():
            print(f"\nüîß –û–±—É—á–µ–Ω–∏–µ {name.upper()}...")
            start_time = time.time()
            
            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            model.fit(X_train, y_train)
            train_time = time.time() - start_time
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            y_pred = model.predict(X_test)
            y_proba = model.predict_proba(X_test)[:, 1]
            
            # –î–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred)
            recall = recall_score(y_test, y_pred)
            f1 = f1_score(y_test, y_pred)
            auc = roc_auc_score(y_test, y_proba)
            
            # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
            cv_start = time.time()
            cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')
            cv_time = time.time() - cv_start
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            model_scores[name] = {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'auc_roc': auc,
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std(),
                'train_time': train_time,
                'cv_time': cv_time
            }
            
            trained_models[name] = model
            base_predictions[name] = model.predict_proba(X_train)[:, 1]
            
            print(f"   ‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.4f}")
            print(f"   üìä Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f}")
            print(f"   üéØ AUC-ROC: {auc:.4f}")
            print(f"   üîÑ CV: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}")
            print(f"   ‚è±Ô∏è  –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {train_time:.1f}s | CV: {cv_time:.1f}s")
            
            # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è tree-based –º–æ–¥–µ–ª–µ–π
            if hasattr(model, 'feature_importances_'):
                self.feature_importance[name] = dict(zip(self.feature_names, model.feature_importances_))
        
        self.training_stats['base_models'] = model_scores
        return trained_models, base_predictions
    
    def train_advanced_meta_model(self, base_predictions, y_train):
        """–û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π –º–µ—Ç–∞–º–æ–¥–µ–ª–∏."""
        print(f"\nüéØ –û–ë–£–ß–ï–ù–ò–ï –ü–†–û–î–í–ò–ù–£–¢–û–ô –ú–ï–¢–ê–ú–û–î–ï–õ–ò...")
        
        meta_features = pd.DataFrame(base_predictions)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –º–µ—Ç–∞–º–æ–¥–µ–ª–∏
        from sklearn.preprocessing import PolynomialFeatures
        poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
        meta_features_poly = poly.fit_transform(meta_features)
        
        start_time = time.time()
        
        # –û–±—É—á–µ–Ω–∏–µ —Å –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        cv_scores = cross_val_score(self.meta_model, meta_features_poly, y_train, cv=cv, scoring='accuracy')
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
        self.meta_model.fit(meta_features_poly, y_train)
        
        train_time = time.time() - start_time
        
        print(f"   ‚úÖ Meta CV: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}")
        print(f"   ‚è±Ô∏è  –í—Ä–µ–º—è: {train_time:.1f}s")
        print(f"   üî¢ –ü—Ä–∏–∑–Ω–∞–∫–æ–≤ –º–µ—Ç–∞–º–æ–¥–µ–ª–∏: {meta_features_poly.shape[1]}")
        
        self.training_stats['meta_model'] = {
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std(),
            'train_time': train_time,
            'n_features': meta_features_poly.shape[1]
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º poly transformer
        self.poly_transformer = poly
        
        return self.meta_model
    
    def evaluate_production_ensemble(self, X_test, y_test, trained_models):
        """–ü—Ä–æ–¥—É–∫—Ü–∏–æ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∞–Ω—Å–∞–º–±–ª—è."""
        print(f"\nüìä –î–ï–¢–ê–õ–¨–ù–ê–Ø –û–¶–ï–ù–ö–ê –ê–ù–°–ê–ú–ë–õ–Ø...")
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
        test_base_predictions = {}
        for name, model in trained_models.items():
            test_base_predictions[name] = model.predict_proba(X_test)[:, 1]
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–µ—Ç–∞–º–æ–¥–µ–ª–∏
        meta_test_features = pd.DataFrame(test_base_predictions)
        meta_test_features_poly = self.poly_transformer.transform(meta_test_features)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∞–Ω—Å–∞–º–±–ª—è
        ensemble_predictions = self.meta_model.predict(meta_test_features_poly)
        ensemble_probabilities = self.meta_model.predict_proba(meta_test_features_poly)[:, 1]
        
        # –î–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        accuracy = accuracy_score(y_test, ensemble_predictions)
        precision = precision_score(y_test, ensemble_predictions)
        recall = recall_score(y_test, ensemble_predictions)
        f1 = f1_score(y_test, ensemble_predictions)
        auc = roc_auc_score(y_test, ensemble_probabilities)
        
        print(f"üéØ –§–ò–ù–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–°–ê–ú–ë–õ–Ø:")
        print(f"   üìä Accuracy:  {accuracy:.4f}")
        print(f"   üìà Precision: {precision:.4f}")
        print(f"   üìâ Recall:    {recall:.4f}")
        print(f"   ‚öñÔ∏è  F1-Score:  {f1:.4f}")
        print(f"   üé≤ AUC-ROC:   {auc:.4f}")
        
        print(f"\nüìã –î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢:")
        print(classification_report(y_test, ensemble_predictions, digits=4))
        
        # Confusion Matrix
        cm = confusion_matrix(y_test, ensemble_predictions)
        print(f"\nüé≠ CONFUSION MATRIX:")
        print(f"   True Neg: {cm[0,0]:>6,} | False Pos: {cm[0,1]:>6,}")
        print(f"   False Neg: {cm[1,0]:>5,} | True Pos:  {cm[1,1]:>6,}")
        
        self.training_stats['ensemble'] = {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'auc_roc': auc,
            'confusion_matrix': cm.tolist()
        }
        
        return ensemble_predictions, ensemble_probabilities, test_base_predictions
    
    def save_production_models(self, trained_models, feature_names):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ü–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."""
        print(f"\nüíæ –°–û–•–†–ê–ù–ï–ù–ò–ï –ü–†–û–î–£–ö–¶–ò–û–ù–ù–´–• –ú–û–î–ï–õ–ï–ô...")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_dir = self.output_dir / f"{timestamp}_production_full"
        model_dir.mkdir(exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
        for name, model in trained_models.items():
            joblib.dump(model, model_dir / f"{name}_intraday.joblib")
            print(f"   ‚úÖ {name}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–º–æ–¥–µ–ª–∏ –∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        joblib.dump(self.meta_model, model_dir / "meta_intraday.joblib")
        joblib.dump(self.scaler, model_dir / "scaler.joblib")
        joblib.dump(feature_names, model_dir / "feature_names.joblib")
        joblib.dump(self.poly_transformer, model_dir / "poly_transformer.joblib")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ –æ–±—É—á–µ–Ω–∏—è
        joblib.dump(self.training_stats, model_dir / "training_stats.joblib")
        joblib.dump(self.feature_importance, model_dir / "feature_importance.joblib")
        
        print(f"   ‚úÖ meta_model, scaler, feature_names")
        print(f"   ‚úÖ poly_transformer, training_stats, feature_importance")
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ latest
        latest_link = self.output_dir / "latest"
        if latest_link.exists():
            latest_link.unlink()
        latest_link.symlink_to(f"{timestamp}_production_full")
        
        print(f"üéâ –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {model_dir}")
        print(f"üîó Latest –æ–±–Ω–æ–≤–ª–µ–Ω: {latest_link}")
        
        return model_dir
    
    def run_full_production_pipeline(self):
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞."""
        print("\n" + "="*80)
        print("üöÄ –ü–û–õ–ù–û–ï –ü–†–û–î–£–ö–¶–ò–û–ù–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï –ê–ù–°–ê–ú–ë–õ–Ø")
        print("   –ë–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π ‚Ä¢ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ ‚Ä¢ –í—Å–µ –¥–∞–Ω–Ω—ã–µ")
        print("="*80)
        
        total_start_time = time.time()
        
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ª–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
        combined_data = self.load_full_dataset()
        
        # 2. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        features_data = self.generate_advanced_features(combined_data)
        
        # 3. –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö —Ç–∞—Ä–≥–µ—Ç–æ–≤
        targets_data = self.create_advanced_targets(features_data)
        
        # 4. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è ML
        feature_cols = [col for col in targets_data.columns 
                       if col not in ['timestamp', 'open', 'high', 'low', 'close', 
                                     'volume', 'target', 'future_return', 'timeframe']]
        
        X = targets_data[feature_cols]
        y = targets_data['target']
        
        print(f"\nüìã –§–ò–ù–ê–õ–¨–ù–´–ï –î–ê–ù–ù–´–ï –î–õ–Ø –û–ë–£–ß–ï–ù–ò–Ø:")
        print(f"   üî¢ –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_cols)}")
        print(f"   üìä –ó–∞–ø–∏—Å–µ–π: {len(X):,}")
        print(f"   üíæ –†–∞–∑–º–µ—Ä: ~{X.memory_usage(deep=True).sum() / 1024**2:.0f} –ú–ë")
        
        self.feature_names = feature_cols
        
        # 5. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        print(f"\nüîß –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")
        normalize_start = time.time()
        X_scaled = self.scaler.fit_transform(X)
        X_scaled = pd.DataFrame(X_scaled, columns=feature_cols, index=X.index)
        normalize_time = time.time() - normalize_start
        print(f"   ‚úÖ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ ({normalize_time:.1f}s)")
        
        # 6. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
        print(f"\nüìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test...")
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=0.15, random_state=42, stratify=y
        )
        
        print(f"   üìà Train: {len(X_train):,} –∑–∞–ø–∏—Å–µ–π ({len(X_train)/len(X)*100:.1f}%)")
        print(f"   üìâ Test:  {len(X_test):,} –∑–∞–ø–∏—Å–µ–π ({len(X_test)/len(X)*100:.1f}%)")
        
        # 7. –û–±—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
        trained_models, base_predictions = self.train_production_models(
            X_train, X_test, y_train, y_test
        )
        
        # 8. –û–±—É—á–µ–Ω–∏–µ –º–µ—Ç–∞–º–æ–¥–µ–ª–∏
        meta_model = self.train_advanced_meta_model(base_predictions, y_train)
        
        # 9. –û—Ü–µ–Ω–∫–∞ –∞–Ω—Å–∞–º–±–ª—è
        ensemble_preds, ensemble_probs, test_base_preds = self.evaluate_production_ensemble(
            X_test, y_test, trained_models
        )
        
        # 10. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
        model_dir = self.save_production_models(trained_models, feature_cols)
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_time = time.time() - total_start_time
        
        print("\n" + "="*80)
        print("üéâ –ü–û–õ–ù–û–ï –ü–†–û–î–£–ö–¶–ò–û–ù–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
        print("="*80)
        print(f"‚è±Ô∏è  –û–ë–©–ï–ï –í–†–ï–ú–Ø: {total_time//60:.0f}–º {total_time%60:.0f}—Å")
        print(f"üìÅ –ú–û–î–ï–õ–ò: {model_dir.name}")
        print(f"üéØ –¢–û–ß–ù–û–°–¢–¨: {self.training_stats['ensemble']['accuracy']:.4f}")
        print(f"üî¢ –ü–†–ò–ó–ù–ê–ö–û–í: {len(feature_cols)}")
        print(f"üìä –î–ê–ù–ù–´–•: {len(X):,} –∑–∞–ø–∏—Å–µ–π")
        print(f"üé≤ AUC-ROC: {self.training_stats['ensemble']['auc_roc']:.4f}")
        print("="*80)
        
        return {
            'model_dir': model_dir,
            'accuracy': self.training_stats['ensemble']['accuracy'],
            'auc_roc': self.training_stats['ensemble']['auc_roc'],
            'total_samples': len(X),
            'feature_count': len(feature_cols),
            'training_time': total_time,
            'stats': self.training_stats
        }


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞."""
    print("üî• –ó–ê–ü–£–°–ö –ü–û–õ–ù–û–ì–û –ü–†–û–î–£–ö–¶–ò–û–ù–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø")
    print("   ‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ü—Ä–æ—Ü–µ—Å—Å –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å 15-30 –º–∏–Ω—É—Ç")
    print("   üíæ –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è: ~2-4 –ì–ë –û–ó–£")
    print()
    
    try:
        trainer = ProductionEnsembleTrainer()
        results = trainer.run_full_production_pipeline()
        
        print(f"\nüèÜ –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
        print(f"   üéØ –¢–æ—á–Ω–æ—Å—Ç—å: {results['accuracy']:.4f}")
        print(f"   üé≤ AUC-ROC: {results['auc_roc']:.4f}")
        print(f"   üìä –û–±—É—á–µ–Ω–æ –Ω–∞: {results['total_samples']:,} –∑–∞–ø–∏—Å—è—Ö")
        print(f"   üî¢ –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {results['feature_count']}")
        print(f"   ‚è±Ô∏è  –í—Ä–µ–º—è: {results['training_time']//60:.0f}–º {results['training_time']%60:.0f}—Å")
        print(f"   üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {results['model_dir'].name}")
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    if success:
        print(f"\nüéâ –ü–†–û–î–£–ö–¶–ò–û–ù–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–û!")
        print(f"üöÄ –ù–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –≥–æ—Ç–æ–≤—ã –∫ live —Ç–æ—Ä–≥–æ–≤–ª–µ!")
    else:
        print(f"\nüí• –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ò–õ–û–°–¨ –° –û–®–ò–ë–ö–û–ô!")
        sys.exit(1)