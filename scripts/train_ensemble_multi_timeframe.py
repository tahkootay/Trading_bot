#!/usr/bin/env python3
"""
–û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π –Ω–∞ –≤—Å–µ—Ö –∏–º–µ—é—â–∏—Ö—Å—è —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞—Ö –∑–∞ 90 –¥–Ω–µ–π
–ë–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö
"""

import sys
import warnings
from pathlib import Path
import pandas as pd
import numpy as np
from datetime import datetime
import joblib
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.linear_model import LogisticRegression

# ML Models
from sklearn.ensemble import RandomForestClassifier
try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
    print("‚ö†Ô∏è Warning: LightGBM not available, will use GradientBoosting instead")

import xgboost as xgb
import catboost as cb
from sklearn.ensemble import GradientBoostingClassifier

# Technical Analysis
import ta
from ta.momentum import RSIIndicator
from ta.trend import MACD, EMAIndicator
from ta.volatility import BollingerBands

warnings.filterwarnings('ignore')


class MultiTimeframeEnsembleTrainer:
    """–¢—Ä–µ–Ω–µ—Ä –∞–Ω—Å–∞–º–±–ª—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤."""
    
    def __init__(self, output_dir: str = "models/ensemble_live"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # –ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏
        self.base_models = {}
        
        # Random Forest
        self.base_models['random_forest'] = RandomForestClassifier(
            n_estimators=300,  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
            max_depth=12,
            min_samples_split=20,
            min_samples_leaf=10,
            max_features='sqrt',
            bootstrap=True,
            random_state=42,
            n_jobs=-1
        )
        
        # LightGBM –∏–ª–∏ GradientBoosting
        if LIGHTGBM_AVAILABLE:
            self.base_models['lightgbm'] = lgb.LGBMClassifier(
                n_estimators=300,
                max_depth=10,
                learning_rate=0.05,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                num_leaves=31,
                subsample=0.8,
                colsample_bytree=0.8,
                reg_alpha=0.1,
                reg_lambda=0.1,
                random_state=42,
                n_jobs=-1,
                verbose=-1
            )
        else:
            self.base_models['gradient_boosting'] = GradientBoostingClassifier(
                n_estimators=300,
                max_depth=10,
                learning_rate=0.05,
                subsample=0.8,
                random_state=42
            )
        
        # XGBoost
        self.base_models['xgboost'] = xgb.XGBClassifier(
            n_estimators=300,
            max_depth=10,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            reg_alpha=0.1,
            reg_lambda=0.1,
            random_state=42,
            n_jobs=-1,
            eval_metric='logloss'
        )
        
        # CatBoost
        self.base_models['catboost'] = cb.CatBoostClassifier(
            iterations=300,
            depth=10,
            learning_rate=0.05,
            l2_leaf_reg=3,
            random_state=42,
            verbose=False
        )
        
        # –ú–µ—Ç–∞–º–æ–¥–µ–ª—å
        self.meta_model = LogisticRegression(
            C=1.0,
            random_state=42,
            max_iter=2000
        )
        
        self.scaler = StandardScaler()
        self.feature_names = []
    
    def find_90d_data_files(self) -> dict:
        """–ü–æ–∏—Å–∫ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ —Å 90-–¥–Ω–µ–≤–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏."""
        data_dir = Path("data/bybit_futures")
        
        timeframe_files = {}
        
        for file_path in data_dir.glob("*90d_bybit_futures.csv"):
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–∞–π–º—Ñ—Ä–µ–π–º –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞
            filename = file_path.name
            parts = filename.split("_")
            if len(parts) >= 2:
                timeframe = parts[1]  # –Ω–∞–ø—Ä–∏–º–µ—Ä, "5m", "1h", "1d"
                timeframe_files[timeframe] = file_path
        
        print(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(timeframe_files)} —Ñ–∞–π–ª–æ–≤ –¥–∞–Ω–Ω—ã—Ö:")
        for tf, path in timeframe_files.items():
            print(f"   {tf}: {path.name}")
        
        return timeframe_files
    
    def load_and_combine_data(self, timeframe_files: dict) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤."""
        combined_data = []
        
        for timeframe, file_path in timeframe_files.items():
            try:
                print(f"üìä –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ {timeframe}...")
                df = pd.read_csv(file_path)
                df['timestamp'] = pd.to_datetime(df['timestamp'])
                df['timeframe'] = timeframe
                
                print(f"   ‚úÖ {timeframe}: {len(df):,} –∑–∞–ø–∏—Å–µ–π")
                combined_data.append(df)
                
            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {timeframe}: {e}")
                continue
        
        if not combined_data:
            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –¥–∞–Ω–Ω—ã—Ö")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
        all_data = pd.concat(combined_data, ignore_index=True)
        all_data = all_data.sort_values(['timeframe', 'timestamp']).reset_index(drop=True)
        
        print(f"\nüìà –û–±—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç: {len(all_data):,} –∑–∞–ø–∏—Å–µ–π")
        print(f"üìÖ –ü–µ—Ä–∏–æ–¥: {all_data['timestamp'].min()} ‚Üí {all_data['timestamp'].max()}")
        print(f"‚è∞ –¢–∞–π–º—Ñ—Ä–µ–π–º—ã: {sorted(all_data['timeframe'].unique())}")
        
        return all_data
    
    def generate_features_multi_tf(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º—É–ª—å—Ç–∏—Ç–∞–π–º—Ñ—Ä–µ–π–º –¥–∞–Ω–Ω—ã—Ö.
        """
        print("üîß –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞–º...")
        
        processed_groups = []
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ç–∞–π–º—Ñ—Ä–µ–π–º –æ—Ç–¥–µ–ª—å–Ω–æ
        for timeframe in df['timeframe'].unique():
            tf_data = df[df['timeframe'] == timeframe].copy()
            tf_data = tf_data.sort_values('timestamp').reset_index(drop=True)
            
            print(f"   –û–±—Ä–∞–±–æ—Ç–∫–∞ {timeframe}: {len(tf_data):,} –∑–∞–ø–∏—Å–µ–π...")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            tf_features = self._generate_single_tf_features(tf_data, timeframe)
            
            if tf_features is not None and len(tf_features) > 0:
                processed_groups.append(tf_features)
                print(f"   ‚úÖ {timeframe}: {len(tf_features):,} –∑–∞–ø–∏—Å–µ–π —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏")
        
        if not processed_groups:
            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        all_features = pd.concat(processed_groups, ignore_index=True)
        all_features = all_features.sort_values(['timeframe', 'timestamp']).reset_index(drop=True)
        
        print(f"‚úÖ –ò—Ç–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(all_features):,} –∑–∞–ø–∏—Å–µ–π")
        
        return all_features
    
    def _generate_single_tf_features(self, df: pd.DataFrame, timeframe: str) -> pd.DataFrame:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞."""
        try:
            df_features = df.copy()
            
            # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            df_features['MA5'] = df_features['close'].rolling(5).mean()
            df_features['MA10'] = df_features['close'].rolling(10).mean()
            df_features['MA20'] = df_features['close'].rolling(20).mean()
            
            # RSI
            rsi = RSIIndicator(df_features['close'], window=14)
            df_features['RSI'] = rsi.rsi()
            
            # MACD
            macd = MACD(df_features['close'])
            df_features['MACD'] = macd.macd()
            df_features['MACD_signal'] = macd.macd_signal()
            df_features['MACD_diff'] = df_features['MACD'] - df_features['MACD_signal']
            
            # Bollinger Bands
            bb = BollingerBands(df_features['close'], window=20)
            df_features['BB_hband'] = bb.bollinger_hband()
            df_features['BB_lband'] = bb.bollinger_lband()
            df_features['BB_width'] = bb.bollinger_wband()
            df_features['BB_position'] = (df_features['close'] - df_features['BB_lband']) / (df_features['BB_hband'] - df_features['BB_lband'])
            
            # Volume –ø—Ä–∏–∑–Ω–∞–∫–∏
            df_features['vol_change'] = df_features['volume'].pct_change()
            df_features['volume_sma'] = df_features['volume'].rolling(20).mean()
            df_features['volume_ratio'] = df_features['volume'] / df_features['volume_sma']
            
            # Price –ø—Ä–∏–∑–Ω–∞–∫–∏
            df_features['price_change'] = df_features['close'].pct_change()
            df_features['price_change_5'] = df_features['close'].pct_change(5)
            df_features['volatility'] = df_features['close'].pct_change().rolling(20).std()
            
            # EMA
            df_features['EMA12'] = EMAIndicator(df_features['close'], window=12).ema_indicator()
            df_features['EMA26'] = EMAIndicator(df_features['close'], window=26).ema_indicator()
            
            # High/Low –¥–∏–∞–ø–∞–∑–æ–Ω—ã
            df_features['high_low_pct'] = (df_features['high'] - df_features['low']) / df_features['close']
            df_features['close_to_high'] = (df_features['high'] - df_features['close']) / df_features['close']
            df_features['close_to_low'] = (df_features['close'] - df_features['low']) / df_features['close']
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–∞–π–º—Ñ—Ä–µ–π–º –∫–∞–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
            timeframe_mapping = {
                '1m': 1, '5m': 5, '15m': 15, '30m': 30, '60m': 60, '1h': 60,
                '120m': 120, '240m': 240, '720m': 720, 'D': 1440
            }
            df_features['timeframe_minutes'] = timeframe_mapping.get(timeframe, 5)
            
            # –£–¥–∞–ª—è–µ–º NaN
            df_features = df_features.dropna()
            
            return df_features
            
        except Exception as e:
            print(f"      ‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {timeframe}: {e}")
            return None
    
    def create_targets_multi_tf(self, df: pd.DataFrame) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–æ–≤ —Å —É—á–µ—Ç–æ–º —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤."""
        print("üéØ –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–æ–≤...")
        
        # –†–∞–∑–Ω—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –ø—Ä–æ–≥–Ω–æ–∑–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤
        horizon_mapping = {
            '1m': 15,    # 15 –º–∏–Ω—É—Ç –≤–ø–µ—Ä–µ–¥
            '5m': 6,     # 30 –º–∏–Ω—É—Ç –≤–ø–µ—Ä–µ–¥  
            '15m': 4,    # 1 —á–∞—Å –≤–ø–µ—Ä–µ–¥
            '30m': 4,    # 2 —á–∞—Å–∞ –≤–ø–µ—Ä–µ–¥
            '60m': 4,    # 4 —á–∞—Å–∞ –≤–ø–µ—Ä–µ–¥
            '120m': 4,   # 8 —á–∞—Å–æ–≤ –≤–ø–µ—Ä–µ–¥
            '240m': 3,   # 12 —á–∞—Å–æ–≤ –≤–ø–µ—Ä–µ–¥
            '720m': 2,   # 1 –¥–µ–Ω—å –≤–ø–µ—Ä–µ–¥
            'D': 2       # 2 –¥–Ω—è –≤–ø–µ—Ä–µ–¥
        }
        
        # –†–∞–∑–Ω—ã–µ –ø–æ—Ä–æ–≥–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤
        threshold_mapping = {
            '1m': 0.001,   # 0.1%
            '5m': 0.002,   # 0.2%
            '15m': 0.005,  # 0.5%
            '30m': 0.008,  # 0.8%
            '60m': 0.01,   # 1.0%
            '120m': 0.015, # 1.5%
            '240m': 0.02,  # 2.0%
            '720m': 0.03,  # 3.0%
            'D': 0.05      # 5.0%
        }
        
        processed_groups = []
        
        for timeframe in df['timeframe'].unique():
            tf_data = df[df['timeframe'] == timeframe].copy()
            
            future_bars = horizon_mapping.get(timeframe, 5)
            threshold = threshold_mapping.get(timeframe, 0.002)
            
            # –°–æ–∑–¥–∞–µ–º —Ç–∞—Ä–≥–µ—Ç –¥–ª—è —ç—Ç–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
            tf_data['future_return'] = tf_data['close'].shift(-future_bars) / tf_data['close'] - 1
            tf_data['target'] = (tf_data['future_return'] > threshold).astype(int)
            
            # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ –±–µ–∑ —Ç–∞—Ä–≥–µ—Ç–æ–≤
            tf_data = tf_data[tf_data['future_return'].notna()].copy()
            
            if len(tf_data) > 0:
                processed_groups.append(tf_data)
                
                target_dist = tf_data['target'].value_counts().sort_index()
                print(f"   {timeframe}: {len(tf_data):,} –∑–∞–ø–∏—Å–µ–π | Threshold: {threshold:.1%} | BUY: {target_dist.get(1, 0):,} ({target_dist.get(1, 0)/len(tf_data)*100:.1f}%)")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
        all_targets = pd.concat(processed_groups, ignore_index=True)
        
        total_dist = all_targets['target'].value_counts().sort_index()
        print(f"\nüìä –û–±—â–µ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–æ–≤:")
        for target, count in total_dist.items():
            print(f"   {target}: {count:,} ({count/len(all_targets)*100:.1f}%)")
        
        return all_targets
    
    def train_ensemble_pipeline(self):
        """–ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è –∞–Ω—Å–∞–º–±–ª—è."""
        print("üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –º—É–ª—å—Ç–∏—Ç–∞–π–º—Ñ—Ä–µ–π–º –¥–∞–Ω–Ω—ã—Ö...")
        print("=" * 60)
        
        # 1. –ù–∞–π—Ç–∏ –∏ –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
        timeframe_files = self.find_90d_data_files()
        if not timeframe_files:
            raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ —Å –¥–∞–Ω–Ω—ã–º–∏")
        
        combined_data = self.load_and_combine_data(timeframe_files)
        
        # 2. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        features_data = self.generate_features_multi_tf(combined_data)
        
        # 3. –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–æ–≤
        targets_data = self.create_targets_multi_tf(features_data)
        
        # 4. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è ML
        feature_cols = [col for col in targets_data.columns 
                       if col not in ['timestamp', 'open', 'high', 'low', 'close', 
                                     'volume', 'target', 'future_return', 'timeframe']]
        
        X = targets_data[feature_cols]
        y = targets_data['target']
        
        print(f"\nüìã –ò—Ç–æ–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {len(feature_cols)}")
        print(f"üìä –†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(X):,} –∑–∞–ø–∏—Å–µ–π")
        self.feature_names = feature_cols
        
        # 5. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        print("üîß –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        X_scaled = self.scaler.fit_transform(X)
        X_scaled = pd.DataFrame(X_scaled, columns=feature_cols, index=X.index)
        
        # 6. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=0.2, random_state=42, stratify=y
        )
        
        print(f"üìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ: Train: {len(X_train):,}, Test: {len(X_test):,}")
        
        # 7. –û–±—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
        trained_models, base_predictions = self.train_base_models(X_train, y_train)
        
        # 8. –û–±—É—á–µ–Ω–∏–µ –º–µ—Ç–∞–º–æ–¥–µ–ª–∏
        meta_model = self.train_meta_model(base_predictions, y_train)
        
        # 9. –û—Ü–µ–Ω–∫–∞ –∞–Ω—Å–∞–º–±–ª—è
        ensemble_preds, ensemble_probs, test_base_preds = self.evaluate_ensemble(
            X_test, y_test, trained_models
        )
        
        # 10. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
        model_dir = self.save_models(trained_models, feature_cols)
        
        print("\n" + "=" * 60)
        print("üéâ –û–±—É—á–µ–Ω–∏–µ –º—É–ª—å—Ç–∏—Ç–∞–π–º—Ñ—Ä–µ–π–º –∞–Ω—Å–∞–º–±–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"üìÅ –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {model_dir}")
        print(f"üéØ –§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {accuracy_score(y_test, ensemble_preds):.4f}")
        print(f"üìä –†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {len(X_train):,} –∑–∞–ø–∏—Å–µ–π")
        print(f"üî¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤: {len(timeframe_files)}")
        
        return {
            'model_dir': model_dir,
            'accuracy': accuracy_score(y_test, ensemble_preds),
            'feature_names': feature_cols,
            'timeframes_used': list(timeframe_files.keys()),
            'total_samples': len(X)
        }
    
    def train_base_models(self, X_train: pd.DataFrame, y_train: pd.Series) -> tuple:
        """–û–±—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π."""
        print("\nü§ñ –û–±—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π...")
        
        trained_models = {}
        base_predictions = {}
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—é –¥–ª—è –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–æ–π –æ—Ü–µ–Ω–∫–∏
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        
        for name, model in self.base_models.items():
            print(f"   –û–±—É—á–µ–Ω–∏–µ {name}...")
            
            # –û–±—É—á–µ–Ω–∏–µ
            model.fit(X_train, y_train)
            
            # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
            cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')
            print(f"   ‚úÖ {name}: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            trained_models[name] = model
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –º–µ—Ç–∞–º–æ–¥–µ–ª–∏
            base_predictions[name] = model.predict_proba(X_train)[:, 1]
        
        return trained_models, base_predictions
    
    def train_meta_model(self, base_predictions: dict, y_train: pd.Series):
        """–û–±—É—á–µ–Ω–∏–µ –º–µ—Ç–∞–º–æ–¥–µ–ª–∏."""
        print("\nüéØ –û–±—É—á–µ–Ω–∏–µ –º–µ—Ç–∞–º–æ–¥–µ–ª–∏ (Logistic Regression)...")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏—á–µ–π –¥–ª—è –º–µ—Ç–∞–º–æ–¥–µ–ª–∏
        meta_features = pd.DataFrame(base_predictions)
        
        # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –º–µ—Ç–∞–º–æ–¥–µ–ª–∏
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        cv_scores = cross_val_score(self.meta_model, meta_features, y_train, cv=cv, scoring='accuracy')
        
        # –û–±—É—á–µ–Ω–∏–µ
        self.meta_model.fit(meta_features, y_train)
        
        print(f"   ‚úÖ Meta-model: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}")
        
        return self.meta_model
    
    def evaluate_ensemble(self, X_test: pd.DataFrame, y_test: pd.Series, trained_models: dict):
        """–û—Ü–µ–Ω–∫–∞ –∞–Ω—Å–∞–º–±–ª—è."""
        print("\nüìä –û—Ü–µ–Ω–∫–∞ –∞–Ω—Å–∞–º–±–ª—è...")
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
        test_base_predictions = {}
        for name, model in trained_models.items():
            test_base_predictions[name] = model.predict_proba(X_test)[:, 1]
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–µ—Ç–∞–º–æ–¥–µ–ª–∏
        meta_test_features = pd.DataFrame(test_base_predictions)
        ensemble_predictions = self.meta_model.predict(meta_test_features)
        ensemble_probabilities = self.meta_model.predict_proba(meta_test_features)[:, 1]
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        accuracy = accuracy_score(y_test, ensemble_predictions)
        print(f"üéØ –¢–æ—á–Ω–æ—Å—Ç—å –∞–Ω—Å–∞–º–±–ª—è: {accuracy:.4f}")
        
        print("\nüìã –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç:")
        print(classification_report(y_test, ensemble_predictions))
        
        return ensemble_predictions, ensemble_probabilities, test_base_predictions
    
    def save_models(self, trained_models: dict, feature_names: list):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π."""
        print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_dir = self.output_dir / f"{timestamp}_multi_tf"
        model_dir.mkdir(exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
        for name, model in trained_models.items():
            joblib.dump(model, model_dir / f"{name}_intraday.joblib")
            print(f"   ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω {name}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–º–æ–¥–µ–ª–∏ –∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        joblib.dump(self.meta_model, model_dir / "meta_intraday.joblib")
        joblib.dump(self.scaler, model_dir / "scaler.joblib")
        joblib.dump(feature_names, model_dir / "feature_names.joblib")
        print(f"   ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω—ã meta_model, scaler, feature_names")
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Å—ã–ª–∫–∏ latest
        latest_link = self.output_dir / "latest"
        if latest_link.exists():
            latest_link.unlink()
        latest_link.symlink_to(f"{timestamp}_multi_tf")
        
        print(f"üéâ –í—Å–µ –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {model_dir}")
        
        return model_dir


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    trainer = MultiTimeframeEnsembleTrainer()
    
    try:
        results = trainer.train_ensemble_pipeline()
        
        print(f"\nüéØ –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
        print(f"   üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –º–æ–¥–µ–ª–µ–π: {results['model_dir']}")
        print(f"   üéØ –¢–æ—á–Ω–æ—Å—Ç—å: {results['accuracy']:.4f}")
        print(f"   üî¢ –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(results['feature_names'])}")
        print(f"   ‚è∞ –¢–∞–π–º—Ñ—Ä–µ–π–º–æ–≤: {len(results['timeframes_used'])} ({', '.join(results['timeframes_used'])})")
        print(f"   üìä –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {results['total_samples']:,} –∑–∞–ø–∏—Å–µ–π")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")
        return False
    
    return True


if __name__ == "__main__":
    success = main()
    if success:
        print("\nüéâ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–ï–®–ù–û!")
    else:
        print("\n‚ùå –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ò–õ–û–°–¨ –° –û–®–ò–ë–ö–û–ô!")