#!/usr/bin/env python3
"""
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –Ω–∞ –≤—Å–µ—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞—Ö –∑–∞ 90 –¥–Ω–µ–π
–° –≤—ã–±–æ—Ä–∫–æ–π –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞
"""

import sys
import warnings
from pathlib import Path
import pandas as pd
import numpy as np
from datetime import datetime
import joblib
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, accuracy_score
from sklearn.linear_model import LogisticRegression

# ML Models
from sklearn.ensemble import RandomForestClassifier
try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False

import xgboost as xgb
import catboost as cb

# Technical Analysis
from ta.momentum import RSIIndicator
from ta.trend import MACD, EMAIndicator
from ta.volatility import BollingerBands

warnings.filterwarnings('ignore')


class OptimizedEnsembleTrainer:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä –∞–Ω—Å–∞–º–±–ª—è."""
    
    def __init__(self, max_samples_per_tf: int = 10000, output_dir: str = "models/ensemble_live"):
        self.max_samples_per_tf = max_samples_per_tf
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ (–º–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)
        self.base_models = {
            'random_forest': RandomForestClassifier(
                n_estimators=100,  # –£–º–µ–Ω—å—à–µ–Ω–æ
                max_depth=8,
                min_samples_split=10,
                random_state=42,
                n_jobs=-1
            ),
            'xgboost': xgb.XGBClassifier(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                random_state=42,
                n_jobs=-1,
                eval_metric='logloss'
            ),
            'catboost': cb.CatBoostClassifier(
                iterations=100,
                depth=6,
                learning_rate=0.1,
                random_state=42,
                verbose=False
            )
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º LightGBM –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        if LIGHTGBM_AVAILABLE:
            self.base_models['lightgbm'] = lgb.LGBMClassifier(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                random_state=42,
                n_jobs=-1,
                verbose=-1
            )
        
        self.meta_model = LogisticRegression(C=1.0, random_state=42, max_iter=1000)
        self.scaler = StandardScaler()
        self.feature_names = []
    
    def load_optimized_data(self) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –≤—ã–±–æ—Ä–∫–æ–π –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏."""
        print("üìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π...")
        
        data_dir = Path("data/bybit_futures")
        combined_data = []
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤ (–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–µ = –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö)
        priority_mapping = {
            '5m': 1.0,    # 100% –¥–∞–Ω–Ω—ã—Ö
            '15m': 0.8,   # 80% –¥–∞–Ω–Ω—ã—Ö  
            '1m': 0.3,    # 30% –¥–∞–Ω–Ω—ã—Ö (—Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ)
            '60m': 0.7,   # 70% –¥–∞–Ω–Ω—ã—Ö
            '240m': 1.0,  # 100% –¥–∞–Ω–Ω—ã—Ö
            'D': 1.0,     # 100% –¥–∞–Ω–Ω—ã—Ö
            '120m': 1.0,  # 100% –¥–∞–Ω–Ω—ã—Ö
            '720m': 1.0   # 100% –¥–∞–Ω–Ω—ã—Ö
        }
        
        total_loaded = 0
        
        for file_path in sorted(data_dir.glob("*90d_bybit_futures.csv")):
            timeframe = file_path.name.split("_")[1]
            
            try:
                df = pd.read_csv(file_path)
                df['timestamp'] = pd.to_datetime(df['timestamp'])
                
                original_count = len(df)
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤—ã–±–æ—Ä–∫—É –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                sample_ratio = priority_mapping.get(timeframe, 0.5)
                max_samples = int(self.max_samples_per_tf * sample_ratio)
                
                if len(df) > max_samples:
                    # –°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
                    df = df.sample(n=max_samples, random_state=42).sort_values('timestamp')
                
                df['timeframe'] = timeframe
                combined_data.append(df)
                
                total_loaded += len(df)
                print(f"   {timeframe}: {len(df):,}/{original_count:,} –∑–∞–ø–∏—Å–µ–π ({len(df)/original_count*100:.1f}%)")
                
            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞ {timeframe}: {e}")
        
        if not combined_data:
            raise ValueError("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        
        all_data = pd.concat(combined_data, ignore_index=True)
        all_data = all_data.sort_values(['timeframe', 'timestamp']).reset_index(drop=True)
        
        print(f"\n‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {total_loaded:,} –∑–∞–ø–∏—Å–µ–π")
        print(f"üìÖ –ü–µ—Ä–∏–æ–¥: {all_data['timestamp'].min()} ‚Üí {all_data['timestamp'].max()}")
        print(f"‚è∞ –¢–∞–π–º—Ñ—Ä–µ–π–º—ã: {sorted(all_data['timeframe'].unique())}")
        
        return all_data
    
    def generate_features_fast(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ë—ã—Å—Ç—Ä–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤."""
        print("‚ö° –ë—ã—Å—Ç—Ä–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        processed_groups = []
        
        for timeframe in df['timeframe'].unique():
            tf_data = df[df['timeframe'] == timeframe].copy()
            tf_data = tf_data.sort_values('timestamp').reset_index(drop=True)
            
            # –ë–∞–∑–æ–≤—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            tf_data['MA5'] = tf_data['close'].rolling(5).mean()
            tf_data['MA10'] = tf_data['close'].rolling(10).mean()
            tf_data['MA20'] = tf_data['close'].rolling(20).mean()
            
            # RSI
            tf_data['RSI'] = RSIIndicator(tf_data['close'], window=14).rsi()
            
            # MACD
            macd = MACD(tf_data['close'])
            tf_data['MACD'] = macd.macd()
            tf_data['MACD_signal'] = macd.macd_signal()
            tf_data['MACD_diff'] = tf_data['MACD'] - tf_data['MACD_signal']
            
            # Bollinger Bands
            bb = BollingerBands(tf_data['close'], window=20)
            tf_data['BB_hband'] = bb.bollinger_hband()
            tf_data['BB_lband'] = bb.bollinger_lband()
            tf_data['BB_width'] = bb.bollinger_wband()
            tf_data['BB_position'] = (tf_data['close'] - tf_data['BB_lband']) / (tf_data['BB_hband'] - tf_data['BB_lband'])
            
            # Volume –∏ Price
            tf_data['vol_change'] = tf_data['volume'].pct_change()
            tf_data['volume_sma'] = tf_data['volume'].rolling(20).mean()
            tf_data['volume_ratio'] = tf_data['volume'] / tf_data['volume_sma']
            tf_data['price_change'] = tf_data['close'].pct_change()
            tf_data['price_change_5'] = tf_data['close'].pct_change(5)
            tf_data['volatility'] = tf_data['close'].pct_change().rolling(20).std()
            
            # EMA
            tf_data['EMA12'] = EMAIndicator(tf_data['close'], window=12).ema_indicator()
            tf_data['EMA26'] = EMAIndicator(tf_data['close'], window=26).ema_indicator()
            
            # High/Low
            tf_data['high_low_pct'] = (tf_data['high'] - tf_data['low']) / tf_data['close']
            tf_data['close_to_high'] = (tf_data['high'] - tf_data['close']) / tf_data['close']
            tf_data['close_to_low'] = (tf_data['close'] - tf_data['low']) / tf_data['close']
            
            # Timeframe encoding
            timeframe_mapping = {'1m': 1, '5m': 5, '15m': 15, '60m': 60, '120m': 120, '240m': 240, '720m': 720, 'D': 1440}
            tf_data['timeframe_minutes'] = timeframe_mapping.get(timeframe, 5)
            
            # –£–±–∏—Ä–∞–µ–º NaN
            tf_data = tf_data.dropna()
            
            if len(tf_data) > 0:
                processed_groups.append(tf_data)
                print(f"   {timeframe}: {len(tf_data):,} –∑–∞–ø–∏—Å–µ–π —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏")
        
        all_features = pd.concat(processed_groups, ignore_index=True)
        print(f"‚úÖ –ò—Ç–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(all_features):,} –∑–∞–ø–∏—Å–µ–π")
        
        return all_features
    
    def create_targets_fast(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ë—ã—Å—Ç—Ä–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–æ–≤."""
        print("üéØ –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–æ–≤...")
        
        # –ï–¥–∏–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –≤—Å–µ—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤ (—É–ø—Ä–æ—â–µ–Ω–∏–µ)
        future_bars = 5
        threshold = 0.005  # 0.5%
        
        processed_groups = []
        
        for timeframe in df['timeframe'].unique():
            tf_data = df[df['timeframe'] == timeframe].copy()
            
            tf_data['future_return'] = tf_data['close'].shift(-future_bars) / tf_data['close'] - 1
            tf_data['target'] = (tf_data['future_return'] > threshold).astype(int)
            tf_data = tf_data[tf_data['future_return'].notna()].copy()
            
            if len(tf_data) > 0:
                processed_groups.append(tf_data)
                
                target_dist = tf_data['target'].value_counts().sort_index()
                buy_pct = target_dist.get(1, 0) / len(tf_data) * 100
                print(f"   {timeframe}: {len(tf_data):,} –∑–∞–ø–∏—Å–µ–π | BUY: {target_dist.get(1, 0):,} ({buy_pct:.1f}%)")
        
        all_targets = pd.concat(processed_groups, ignore_index=True)
        
        total_dist = all_targets['target'].value_counts().sort_index()
        print(f"\nüìä –û–±—â–µ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:")
        for target, count in total_dist.items():
            print(f"   {target}: {count:,} ({count/len(all_targets)*100:.1f}%)")
        
        return all_targets
    
    def train_fast_pipeline(self):
        """–ë—ã—Å—Ç—Ä—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è."""
        print("üöÄ –ë–´–°–¢–†–û–ï –û–ë–£–ß–ï–ù–ò–ï –ê–ù–°–ê–ú–ë–õ–Ø –ù–ê –í–°–ï–• –¢–ê–ô–ú–§–†–ï–ô–ú–ê–•")
        print("=" * 60)
        
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        combined_data = self.load_optimized_data()
        
        # 2. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        features_data = self.generate_features_fast(combined_data)
        
        # 3. –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–æ–≤
        targets_data = self.create_targets_fast(features_data)
        
        # 4. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è ML
        feature_cols = [col for col in targets_data.columns 
                       if col not in ['timestamp', 'open', 'high', 'low', 'close', 
                                     'volume', 'target', 'future_return', 'timeframe']]
        
        X = targets_data[feature_cols]
        y = targets_data['target']
        
        print(f"\nüìã –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_cols)}")
        print(f"üìä –î–∞—Ç–∞—Å–µ—Ç: {len(X):,} –∑–∞–ø–∏—Å–µ–π")
        self.feature_names = feature_cols
        
        # 5. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        X_scaled = self.scaler.fit_transform(X)
        X_scaled = pd.DataFrame(X_scaled, columns=feature_cols, index=X.index)
        
        # 6. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=0.2, random_state=42, stratify=y
        )
        
        print(f"üìä Train: {len(X_train):,}, Test: {len(X_test):,}")
        
        # 7. –û–±—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
        print("\nü§ñ –û–±—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π...")
        trained_models = {}
        base_predictions = {}
        
        for name, model in self.base_models.items():
            print(f"   –û–±—É—á–µ–Ω–∏–µ {name}...")
            model.fit(X_train, y_train)
            
            # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–µ
            test_pred = model.predict(X_test)
            test_acc = accuracy_score(y_test, test_pred)
            print(f"   ‚úÖ {name}: {test_acc:.4f}")
            
            trained_models[name] = model
            base_predictions[name] = model.predict_proba(X_train)[:, 1]
        
        # 8. –û–±—É—á–µ–Ω–∏–µ –º–µ—Ç–∞–º–æ–¥–µ–ª–∏
        print("\nüéØ –û–±—É—á–µ–Ω–∏–µ –º–µ—Ç–∞–º–æ–¥–µ–ª–∏...")
        meta_features = pd.DataFrame(base_predictions)
        self.meta_model.fit(meta_features, y_train)
        
        # 9. –û—Ü–µ–Ω–∫–∞ –∞–Ω—Å–∞–º–±–ª—è
        test_base_predictions = {}
        for name, model in trained_models.items():
            test_base_predictions[name] = model.predict_proba(X_test)[:, 1]
        
        meta_test_features = pd.DataFrame(test_base_predictions)
        ensemble_predictions = self.meta_model.predict(meta_test_features)
        ensemble_accuracy = accuracy_score(y_test, ensemble_predictions)
        
        print(f"\nüéØ –†–ï–ó–£–õ–¨–¢–ê–¢ –ê–ù–°–ê–ú–ë–õ–Ø: {ensemble_accuracy:.4f}")
        print("\nüìã –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç:")
        print(classification_report(y_test, ensemble_predictions))
        
        # 10. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        model_dir = self.save_models(trained_models, feature_cols)
        
        print("\n" + "=" * 60)
        print("üéâ –ë–´–°–¢–†–û–ï –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
        print(f"üìÅ –ú–æ–¥–µ–ª–∏: {model_dir}")
        print(f"üéØ –¢–æ—á–Ω–æ—Å—Ç—å: {ensemble_accuracy:.4f}")
        print(f"üìä –û–±—É—á–µ–Ω–æ –Ω–∞: {len(X):,} –∑–∞–ø–∏—Å–µ–π")
        
        return {
            'model_dir': model_dir,
            'accuracy': ensemble_accuracy,
            'total_samples': len(X),
            'feature_count': len(feature_cols)
        }
    
    def save_models(self, trained_models: dict, feature_names: list):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_dir = self.output_dir / f"{timestamp}_multi_tf_optimized"
        model_dir.mkdir(exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        for name, model in trained_models.items():
            joblib.dump(model, model_dir / f"{name}_intraday.joblib")
        
        joblib.dump(self.meta_model, model_dir / "meta_intraday.joblib")
        joblib.dump(self.scaler, model_dir / "scaler.joblib")
        joblib.dump(feature_names, model_dir / "feature_names.joblib")
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ latest
        latest_link = self.output_dir / "latest"
        if latest_link.exists():
            latest_link.unlink()
        latest_link.symlink_to(f"{timestamp}_multi_tf_optimized")
        
        print(f"üíæ –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {model_dir}")
        return model_dir


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    print("‚ö° –ó–ê–ü–£–°–ö –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø")
    
    trainer = OptimizedEnsembleTrainer(max_samples_per_tf=15000)
    
    try:
        results = trainer.train_fast_pipeline()
        
        print(f"\nüèÜ –§–ò–ù–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
        print(f"   üéØ –¢–æ—á–Ω–æ—Å—Ç—å: {results['accuracy']:.4f}")
        print(f"   üìä –†–∞–∑–º–µ—Ä –æ–±—É—á–µ–Ω–∏—è: {results['total_samples']:,} –∑–∞–ø–∏—Å–µ–π")
        print(f"   üî¢ –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {results['feature_count']}")
        print(f"   üìÅ –ú–æ–¥–µ–ª–∏: {results['model_dir'].name}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå –û–®–ò–ë–ö–ê: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    if success:
        print("\nüéâ –û–ë–£–ß–ï–ù–ò–ï –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–û!")
    else:
        print("\n‚ùå –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ò–õ–û–°–¨ –° –û–®–ò–ë–ö–û–ô!")