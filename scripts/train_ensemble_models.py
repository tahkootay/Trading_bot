#!/usr/bin/env python3
"""
–û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –∏–∑ 4 –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π + –º–µ—Ç–∞–º–æ–¥–µ–ª—å –¥–ª—è live-—Ç–æ—Ä–≥–æ–≤–ª–∏
–°–æ–≥–ª–∞—Å–Ω–æ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏: RF, LightGBM, XGBoost, CatBoost + Logistic Regression
"""

import sys
import warnings
from pathlib import Path
import pandas as pd
import numpy as np
from datetime import datetime
import joblib
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.linear_model import LogisticRegression

# ML Models
from sklearn.ensemble import RandomForestClassifier
try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
    print("‚ö†Ô∏è Warning: LightGBM not available, will use GradientBoosting instead")

import xgboost as xgb
import catboost as cb
from sklearn.ensemble import GradientBoostingClassifier

# Technical Analysis
import ta
from ta.momentum import RSIIndicator
from ta.trend import MACD, EMAIndicator
from ta.volatility import BollingerBands
# from ta.volume import VolumeSMAIndicator  # Not available in current ta version

warnings.filterwarnings('ignore')

class EnsembleModelTrainer:
    """–¢—Ä–µ–Ω–µ—Ä –∞–Ω—Å–∞–º–±–ª—è –∏–∑ 4 –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π + –º–µ—Ç–∞–º–æ–¥–µ–ª—å."""
    
    def __init__(self, output_dir: str = "models/ensemble_live"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # –ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏
        self.base_models = {}
        
        # Random Forest - –≤—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–µ–Ω
        self.base_models['random_forest'] = RandomForestClassifier(
            n_estimators=200,
            max_depth=10,
            min_samples_split=20,
            min_samples_leaf=10,
            random_state=42,
            n_jobs=-1
        )
        
        # LightGBM - —Å fallback –Ω–∞ GradientBoosting
        if LIGHTGBM_AVAILABLE:
            self.base_models['lightgbm'] = lgb.LGBMClassifier(
                n_estimators=200,
                max_depth=8,
                learning_rate=0.1,
                num_leaves=31,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                n_jobs=-1,
                verbose=-1
            )
        else:
            print("‚ö†Ô∏è  Using GradientBoostingClassifier instead of LightGBM")
            self.base_models['gradient_boosting'] = GradientBoostingClassifier(
                n_estimators=200,
                max_depth=8,
                learning_rate=0.1,
                subsample=0.8,
                random_state=42
            )
        
        # XGBoost
        self.base_models['xgboost'] = xgb.XGBClassifier(
            n_estimators=200,
            max_depth=8,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            n_jobs=-1,
            eval_metric='logloss'
        )
        
        # CatBoost
        self.base_models['catboost'] = cb.CatBoostClassifier(
            iterations=200,
            depth=8,
            learning_rate=0.1,
            l2_leaf_reg=3,
            random_state=42,
            verbose=False
        )
        
        # –ú–µ—Ç–∞–º–æ–¥–µ–ª—å
        self.meta_model = LogisticRegression(
            C=1.0,
            random_state=42,
            max_iter=1000
        )
        
        self.scaler = StandardScaler()
        self.feature_names = []
        
    def generate_features_training(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏—á–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–Ω–æ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏.
        –í–∫–ª—é—á–∞–µ—Ç: MA5/10/20, RSI, MACD, Bollinger Bands, Volume change
        """
        df = df.copy()
        df = df.sort_values('timestamp').reset_index(drop=True)
        
        print("üîß Generating features...")
        
        # Moving Averages (–∫–∞–∫ –≤ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏)
        df['MA5'] = df['close'].rolling(5).mean()
        df['MA10'] = df['close'].rolling(10).mean()
        df['MA20'] = df['close'].rolling(20).mean()
        
        # RSI
        rsi = RSIIndicator(df['close'], window=14)
        df['RSI'] = rsi.rsi()
        
        # MACD
        macd = MACD(df['close'])
        df['MACD'] = macd.macd()
        df['MACD_signal'] = macd.macd_signal()
        df['MACD_diff'] = df['MACD'] - df['MACD_signal']
        
        # Bollinger Bands
        bb = BollingerBands(df['close'], window=20)
        df['BB_hband'] = bb.bollinger_hband()
        df['BB_lband'] = bb.bollinger_lband()
        df['BB_width'] = bb.bollinger_wband()
        df['BB_position'] = (df['close'] - df['BB_lband']) / (df['BB_hband'] - df['BB_lband'])
        
        # Volume change
        df['vol_change'] = df['volume'].pct_change()
        df['volume_sma'] = df['volume'].rolling(20).mean()
        df['volume_ratio'] = df['volume'] / df['volume_sma']
        
        # Additional technical features
        df['price_change'] = df['close'].pct_change()
        df['price_change_5'] = df['close'].pct_change(5)
        df['volatility'] = df['close'].pct_change().rolling(20).std()
        
        # EMA indicators 
        df['EMA12'] = ta.trend.EMAIndicator(df['close'], window=12).ema_indicator()
        df['EMA26'] = ta.trend.EMAIndicator(df['close'], window=26).ema_indicator()
        
        # High/Low ranges
        df['high_low_pct'] = (df['high'] - df['low']) / df['close']
        df['close_to_high'] = (df['high'] - df['close']) / df['close']
        df['close_to_low'] = (df['close'] - df['low']) / df['close']
        
        # Drop NaN values
        df = df.dropna()
        
        print(f"‚úÖ Generated {len([col for col in df.columns if col not in ['timestamp', 'open', 'high', 'low', 'close', 'volume']])} features")
        return df
    
    def create_targets(self, df: pd.DataFrame, future_bars: int = 5, threshold: float = 0.002) -> pd.DataFrame:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –±–∏–Ω–∞—Ä–Ω—ã—Ö —Ç–∞—Ä–≥–µ—Ç–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.
        1 = BUY (—Ü–µ–Ω–∞ –≤—ã—Ä–∞—Å—Ç–µ—Ç –±–æ–ª—å—à–µ —á–µ–º –Ω–∞ threshold –∑–∞ future_bars —Å–≤–µ—á–µ–π)
        0 = NO SIGNAL (—Ü–µ–Ω–∞ –Ω–µ –∏–∑–º–µ–Ω–∏—Ç—Å—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ)
        """
        df = df.copy()
        
        # –ë—É–¥—É—â–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å
        df['future_return'] = df['close'].shift(-future_bars) / df['close'] - 1
        
        # –ë–∏–Ω–∞—Ä–Ω—ã–π —Ç–∞—Ä–≥–µ—Ç: 1 –µ—Å–ª–∏ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å > threshold, –∏–Ω–∞—á–µ 0
        df['target'] = (df['future_return'] > threshold).astype(int)
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ –±–µ–∑ —Ç–∞—Ä–≥–µ—Ç–æ–≤
        df = df[df['future_return'].notna()].copy()
        
        target_distribution = df['target'].value_counts().sort_index()
        print(f"üìä Target distribution:")
        for target, count in target_distribution.items():
            print(f"   {target}: {count:,} ({count/len(df)*100:.1f}%)")
        
        return df
    
    def train_base_models(self, X_train: pd.DataFrame, y_train: pd.Series) -> dict:
        """–û–±—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π."""
        print("\nü§ñ Training base models...")
        
        trained_models = {}
        base_predictions = {}
        
        for name, model in self.base_models.items():
            print(f"   Training {name}...")
            
            # –û–±—É—á–µ–Ω–∏–µ
            model.fit(X_train, y_train)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            cv_scores = cross_val_score(model, X_train, y_train, cv=3, scoring='accuracy')
            print(f"   ‚úÖ {name}: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            trained_models[name] = model
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–ª—è –º–µ—Ç–∞–º–æ–¥–µ–ª–∏ (–Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ)
            base_predictions[name] = model.predict_proba(X_train)[:, 1]
        
        return trained_models, base_predictions
    
    def train_meta_model(self, base_predictions: dict, y_train: pd.Series):
        """–û–±—É—á–µ–Ω–∏–µ –º–µ—Ç–∞–º–æ–¥–µ–ª–∏ –Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π."""
        print("\nüéØ Training meta-model (Logistic Regression)...")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏—á–µ–π –¥–ª—è –º–µ—Ç–∞–º–æ–¥–µ–ª–∏ –∏–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
        meta_features = pd.DataFrame(base_predictions)
        
        # –û–±—É—á–µ–Ω–∏–µ –º–µ—Ç–∞–º–æ–¥–µ–ª–∏
        self.meta_model.fit(meta_features, y_train)
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –º–µ—Ç–∞–º–æ–¥–µ–ª–∏
        cv_scores = cross_val_score(self.meta_model, meta_features, y_train, cv=3, scoring='accuracy')
        print(f"   ‚úÖ Meta-model: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}")
        
        return self.meta_model
    
    def evaluate_ensemble(self, X_test: pd.DataFrame, y_test: pd.Series, trained_models: dict):
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∞–Ω—Å–∞–º–±–ª—è."""
        print("\nüìä Evaluating ensemble performance...")
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ
        test_base_predictions = {}
        for name, model in trained_models.items():
            test_base_predictions[name] = model.predict_proba(X_test)[:, 1]
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–µ—Ç–∞–º–æ–¥–µ–ª–∏
        meta_test_features = pd.DataFrame(test_base_predictions)
        ensemble_predictions = self.meta_model.predict(meta_test_features)
        ensemble_probabilities = self.meta_model.predict_proba(meta_test_features)[:, 1]
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        accuracy = accuracy_score(y_test, ensemble_predictions)
        print(f"üéØ Ensemble Accuracy: {accuracy:.4f}")
        
        print("\nüìã Classification Report:")
        print(classification_report(y_test, ensemble_predictions))
        
        return ensemble_predictions, ensemble_probabilities, test_base_predictions
    
    def save_models(self, trained_models: dict, feature_names: list):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π."""
        print(f"\nüíæ Saving models to {self.output_dir}...")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_dir = self.output_dir / timestamp
        model_dir.mkdir(exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
        for name, model in trained_models.items():
            joblib.dump(model, model_dir / f"{name}_intraday.joblib")
            print(f"   ‚úÖ Saved {name}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–º–æ–¥–µ–ª–∏
        joblib.dump(self.meta_model, model_dir / "meta_intraday.joblib")
        print(f"   ‚úÖ Saved meta_model")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–∫–µ–π–ª–µ—Ä–∞ –∏ —Ñ–∏—á–µ–π
        joblib.dump(self.scaler, model_dir / "scaler.joblib")
        joblib.dump(feature_names, model_dir / "feature_names.joblib")
        print(f"   ‚úÖ Saved scaler and feature_names")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω—é—é –≤–µ—Ä—Å–∏—é
        latest_link = self.output_dir / "latest"
        if latest_link.exists():
            latest_link.unlink()
        latest_link.symlink_to(timestamp)
        
        print(f"üéâ All models saved successfully!")
        return model_dir
    
    def train_full_pipeline(self, data_file: str):
        """–ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è –∞–Ω—Å–∞–º–±–ª—è."""
        print("üöÄ Starting ensemble training pipeline...")
        print("=" * 60)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        print(f"üìä Loading data from {data_file}...")
        df = pd.read_csv(data_file)
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        print(f"   ‚úÖ Loaded {len(df):,} records")
        print(f"   üìÖ Period: {df['timestamp'].min()} ‚Üí {df['timestamp'].max()}")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏—á–µ–π
        df_features = self.generate_features_training(df)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–æ–≤
        df_targets = self.create_targets(df_features)
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        feature_cols = [col for col in df_targets.columns 
                       if col not in ['timestamp', 'open', 'high', 'low', 'close', 
                                     'volume', 'target', 'future_return']]
        
        X = df_targets[feature_cols]
        y = df_targets['target']
        
        print(f"\nüìã Features: {len(feature_cols)}")
        self.feature_names = feature_cols
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ñ–∏—á–µ–π
        X_scaled = self.scaler.fit_transform(X)
        X_scaled = pd.DataFrame(X_scaled, columns=feature_cols, index=X.index)
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=0.2, random_state=42, stratify=y
        )
        
        print(f"üìä Train: {len(X_train):,}, Test: {len(X_test):,}")
        
        # –û–±—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
        trained_models, base_predictions = self.train_base_models(X_train, y_train)
        
        # –û–±—É—á–µ–Ω–∏–µ –º–µ—Ç–∞–º–æ–¥–µ–ª–∏
        meta_model = self.train_meta_model(base_predictions, y_train)
        
        # –û—Ü–µ–Ω–∫–∞ –∞–Ω—Å–∞–º–±–ª—è
        ensemble_preds, ensemble_probs, test_base_preds = self.evaluate_ensemble(
            X_test, y_test, trained_models
        )
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
        model_dir = self.save_models(trained_models, feature_cols)
        
        print("\n" + "=" * 60)
        print("üéâ Ensemble training completed successfully!")
        print(f"üìÅ Models saved to: {model_dir}")
        
        return {
            'model_dir': model_dir,
            'feature_names': feature_cols,
            'accuracy': accuracy_score(y_test, ensemble_preds),
            'base_models': trained_models,
            'meta_model': meta_model
        }

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–Ω—Å–∞–º–±–ª—è."""
    
    # –ü–æ–∏—Å–∫ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    data_files = [
        "data/bybit_futures/SOLUSDT_5m_90d_bybit_futures.csv",
        "data/enhanced/SOLUSDT_5m_90d_enhanced.csv"
    ]
    
    data_file = None
    for file_path in data_files:
        if Path(file_path).exists():
            data_file = file_path
            break
    
    if not data_file:
        print("‚ùå No training data found!")
        print("   Expected files:")
        for file_path in data_files:
            print(f"     {file_path}")
        return
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞ –∏ –∑–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
    trainer = EnsembleModelTrainer()
    results = trainer.train_full_pipeline(data_file)
    
    print(f"\nüéØ Final Results:")
    print(f"   Accuracy: {results['accuracy']:.4f}")
    print(f"   Feature count: {len(results['feature_names'])}")
    print(f"   Model directory: {results['model_dir']}")

if __name__ == "__main__":
    main()