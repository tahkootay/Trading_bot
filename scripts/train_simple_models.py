#!/usr/bin/env python3
"""
–ü—Ä–æ—Å—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ ML –º–æ–¥–µ–ª–µ–π –±–µ–∑ XGBoost (—Ç–æ–ª—å–∫–æ scikit-learn)
"""

import sys
import warnings
from datetime import datetime, timedelta
from pathlib import Path
import pandas as pd
import numpy as np
import click
import json
from typing import Dict, List, Tuple, Optional

# –ü–æ–¥–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π
warnings.filterwarnings('ignore')

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ src –≤ –ø—É—Ç—å
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import joblib

class SimpleMLTrainer:
    """–ü—Ä–æ—Å—Ç–æ–π —Ç—Ä–µ–Ω–µ—Ä ML –º–æ–¥–µ–ª–µ–π –Ω–∞ scikit-learn"""
    
    def __init__(self, output_dir: str = "models"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
    def load_data(self, data_dir: str = "data") -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        data_path = Path(data_dir)
        
        # –ò—â–µ–º —Ñ–∞–π–ª—ã —Å –¥–∞–Ω–Ω—ã–º–∏ (5-–º–∏–Ω—É—Ç–Ω—ã–π —Ç–∞–π–º—Ñ—Ä–µ–π–º)
        data_files = []
        for pattern in ['*5m*.csv', '*_real_*.csv', '*_august_*.csv']:
            data_files.extend(data_path.glob(pattern))
            data_files.extend(data_path.glob(f"blocks/data/*/{pattern}"))
        
        print(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(data_files)} —Ñ–∞–π–ª–æ–≤ –¥–∞–Ω–Ω—ã—Ö")
        
        dfs = []
        for file_path in data_files:
            if '5m' in file_path.name:  # –§–æ–∫—É—Å–∏—Ä—É–µ–º—Å—è –Ω–∞ 5-–º–∏–Ω—É—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                try:
                    df = pd.read_csv(file_path)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
                    required_cols = ['open', 'high', 'low', 'close', 'volume']
                    if all(col in df.columns for col in required_cols):
                        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ –≤—Ä–µ–º–µ–Ω–∏
                        if 'timestamp' in df.columns:
                            df['timestamp'] = pd.to_datetime(df['timestamp'])
                            df = df.set_index('timestamp')
                        
                        dfs.append(df)
                        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω: {file_path.name} ({len(df)} —Å—Ç—Ä–æ–∫)")
                
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file_path.name}: {e}")
        
        if not dfs:
            raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ –æ—á–∏—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        combined_df = pd.concat(dfs, axis=0)
        combined_df = combined_df.sort_index()
        combined_df = combined_df[~combined_df.index.duplicated(keep='first')]
        
        print(f"üìà –û–±—ä–µ–¥–∏–Ω–µ–Ω–æ: {len(combined_df)} –∑–∞–ø–∏—Å–µ–π")
        print(f"üìÖ –ü–µ—Ä–∏–æ–¥: {combined_df.index[0]} - {combined_df.index[-1]}")
        
        return combined_df
    
    def calculate_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–†–∞—Å—á–µ—Ç –ø—Ä–æ—Å—Ç—ã—Ö —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"""
        print("üîß –†–∞—Å—á–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤...")
        
        features_df = df.copy()
        
        # –ü—Ä–æ—Å—Ç—ã–µ —Å–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ
        for period in [5, 10, 20, 50]:
            features_df[f'sma_{period}'] = df['close'].rolling(period).mean()
            features_df[f'price_vs_sma_{period}'] = (df['close'] - features_df[f'sma_{period}']) / features_df[f'sma_{period}']
        
        # RSI (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π)
        delta = df['close'].diff()
        gain = delta.where(delta > 0, 0).rolling(14).mean()
        loss = -delta.where(delta < 0, 0).rolling(14).mean()
        rs = gain / loss
        features_df['rsi'] = 100 - (100 / (1 + rs))
        
        # –ü—Ä–æ—Å—Ç–æ–π MACD
        ema_12 = df['close'].ewm(span=12).mean()
        ema_26 = df['close'].ewm(span=26).mean()
        features_df['macd'] = ema_12 - ema_26
        
        # Bollinger Bands
        sma_20 = df['close'].rolling(20).mean()
        std_20 = df['close'].rolling(20).std()
        features_df['bb_upper'] = sma_20 + (std_20 * 2)
        features_df['bb_lower'] = sma_20 - (std_20 * 2)
        features_df['bb_position'] = (df['close'] - features_df['bb_lower']) / (features_df['bb_upper'] - features_df['bb_lower'])
        
        # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
        features_df['volatility'] = df['close'].pct_change().rolling(20).std()
        
        # –ò–∑–º–µ–Ω–µ–Ω–∏—è —Ü–µ–Ω—ã
        for period in [1, 5, 10, 20]:
            features_df[f'price_change_{period}'] = df['close'].pct_change(period)
        
        # –û–±—ä–µ–º–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        features_df['volume_sma'] = df['volume'].rolling(20).mean()
        features_df['volume_ratio'] = df['volume'] / features_df['volume_sma']
        
        # –ú–∞–∫—Å–∏–º—É–º—ã –∏ –º–∏–Ω–∏–º—É–º—ã
        features_df['high_20'] = df['high'].rolling(20).max()
        features_df['low_20'] = df['low'].rolling(20).min()
        features_df['price_position'] = (df['close'] - features_df['low_20']) / (features_df['high_20'] - features_df['low_20'])
        
        print(f"‚úÖ –†–∞—Å—Å—á–∏—Ç–∞–Ω–æ {len(features_df.columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        return features_df
    
    def create_targets(self, df: pd.DataFrame, forward_periods: int = 12) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö"""
        print(f"üéØ –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö (–ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ {forward_periods} –ø–µ—Ä–∏–æ–¥–æ–≤)...")
        
        target_df = df.copy()
        
        # –ë—É–¥—É—â–∞—è —Ü–µ–Ω–∞
        future_price = df['close'].shift(-forward_periods)
        current_price = df['close']
        
        # –ü—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ
        price_change = (future_price - current_price) / current_price
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ –ø–æ—Ä–æ–≥–∞–º–∏
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç–∏–ª–∏ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∫ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
        up_threshold = price_change.quantile(0.75)    # –¢–æ–ø 25% –¥–≤–∏–∂–µ–Ω–∏–π
        down_threshold = price_change.quantile(0.25)  # –ù–∏–∑ 25% –¥–≤–∏–∂–µ–Ω–∏–π
        
        print(f"üìä –ü–æ—Ä–æ–≥–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: UP > {up_threshold:.3%}, DOWN < {down_threshold:.3%}")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–æ–≤
        target_df['target'] = 1  # Flat (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
        target_df.loc[price_change > up_threshold, 'target'] = 2   # Up
        target_df.loc[price_change < down_threshold, 'target'] = 0  # Down
        
        # –£–¥–∞–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ –±–µ–∑ –±—É–¥—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        target_df = target_df.iloc[:-forward_periods]
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        target_counts = target_df['target'].value_counts().sort_index()
        print(f"üìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
        print(f"  DOWN (0): {target_counts.get(0, 0)} ({target_counts.get(0, 0)/len(target_df)*100:.1f}%)")
        print(f"  FLAT (1): {target_counts.get(1, 0)} ({target_counts.get(1, 0)/len(target_df)*100:.1f}%)")
        print(f"  UP (2):   {target_counts.get(2, 0)} ({target_counts.get(2, 0)/len(target_df)*100:.1f}%)")
        
        return target_df
    
    def prepare_training_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        print("üìã –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...")
        
        # –í—ã–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–∏—Å–∫–ª—é—á–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ OHLCV –∏ —Å–ª—É–∂–µ–±–Ω—ã–µ)
        feature_cols = [col for col in df.columns if col not in 
                       ['open', 'high', 'low', 'close', 'volume', 'turnover', 'target']]
        
        X = df[feature_cols].copy()
        y = df['target'].copy()
        
        # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Å NaN
        valid_mask = ~(X.isna().any(axis=1) | y.isna())
        X = X[valid_mask]
        y = y[valid_mask]
        
        # –ó–∞–º–µ–Ω–∞ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ—Å—Ç–∏ –∏ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è NaN
        X = X.replace([np.inf, -np.inf], np.nan)
        X = X.fillna(X.median())
        
        print(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ: {len(X)} –æ–±—Ä–∞–∑—Ü–æ–≤ —Å {len(X.columns)} –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏")
        print(f"üìã –ü—Ä–∏–∑–Ω–∞–∫–∏: {', '.join(X.columns[:5])}{'...' if len(X.columns) > 5 else ''}")
        
        return X, y
    
    def train_models(self, X: pd.DataFrame, y: pd.Series, model_version: str = None):
        """–û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –ø—Ä–æ—Å—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        print("ü§ñ –û–±—É—á–µ–Ω–∏–µ ML –º–æ–¥–µ–ª–µ–π...")
        
        if model_version is None:
            model_version = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ/—Ç–µ—Å—Ç
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        print(f"üìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:")
        print(f"  –û–±—É—á–µ–Ω–∏–µ: {len(X_train)} –æ–±—Ä–∞–∑—Ü–æ–≤")
        print(f"  –¢–µ—Å—Ç: {len(X_test)} –æ–±—Ä–∞–∑—Ü–æ–≤")
        
        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # –ú–æ–¥–µ–ª–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        models = {
            'random_forest': RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                min_samples_split=5,
                random_state=42,
                n_jobs=-1
            ),
            'gradient_boosting': GradientBoostingClassifier(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                random_state=42
            ),
            'decision_tree': DecisionTreeClassifier(
                max_depth=15,
                min_samples_split=10,
                random_state=42
            )
        }
        
        trained_models = {}
        model_scores = {}
        
        # –û–±—É—á–µ–Ω–∏–µ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
        for model_name, model in models.items():
            print(f"üîÑ –û–±—É—á–µ–Ω–∏–µ {model_name}...")
            
            # –û–±—É—á–µ–Ω–∏–µ
            if 'forest' in model_name or 'tree' in model_name:
                model.fit(X_train, y_train)  # Tree-based models –Ω–µ –Ω—É–∂–¥–∞—é—Ç—Å—è –≤ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–∏
                y_pred = model.predict(X_test)
            else:
                model.fit(X_train_scaled, y_train)
                y_pred = model.predict(X_test_scaled)
            
            # –û—Ü–µ–Ω–∫–∞
            accuracy = accuracy_score(y_test, y_pred)
            
            trained_models[model_name] = model
            model_scores[model_name] = accuracy
            
            print(f"  ‚úÖ {model_name}: —Ç–æ—á–Ω–æ—Å—Ç—å = {accuracy:.3f}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
        model_dir = self.output_dir / model_version
        model_dir.mkdir(exist_ok=True)
        
        for model_name, model in trained_models.items():
            model_path = model_dir / f"{model_name}.joblib"
            joblib.dump(model, model_path)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ scaler –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        joblib.dump(scaler, model_dir / "scaler.joblib")
        joblib.dump(X.columns.tolist(), model_dir / "feature_names.joblib")
        
        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = {
            "model_version": model_version,
            "training_date": datetime.now().isoformat(),
            "samples_trained": len(X_train),
            "samples_tested": len(X_test),
            "features_count": len(X.columns),
            "feature_names": X.columns.tolist(),
            "model_scores": model_scores,
            "ensemble_weights": {
                "random_forest": 0.4,
                "gradient_boosting": 0.4,
                "decision_tree": 0.2
            }
        }
        
        with open(model_dir / "metadata.json", 'w') as f:
            json.dump(metadata, f, indent=2)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–æ–π —Å—Å—ã–ª–∫–∏ –Ω–∞ latest
        latest_link = self.output_dir / "latest"
        if latest_link.exists():
            latest_link.unlink()
        latest_link.symlink_to(model_version)
        
        print(f"üíæ –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {model_dir}")
        print(f"üîó –°–æ–∑–¥–∞–Ω–∞ —Å—Å—ã–ª–∫–∞: {latest_link}")
        
        # –ü–æ–∫–∞–∑–∞—Ç—å –ª—É—á—à–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–¥–ª—è Random Forest)
        if 'random_forest' in trained_models:
            feature_importance = trained_models['random_forest'].feature_importances_
            feature_names = X.columns
            
            # –¢–æ–ø-10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            importance_df = pd.DataFrame({
                'feature': feature_names,
                'importance': feature_importance
            }).sort_values('importance', ascending=False)
            
            print(f"üèÜ –¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
            for i, row in importance_df.head(10).iterrows():
                print(f"  {row['feature']}: {row['importance']:.3f}")
        
        return model_scores

@click.command()
@click.option('--data-dir', '-d', default='data', help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–∞–Ω–Ω—ã–º–∏')
@click.option('--models-dir', '-m', default='models', help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –º–æ–¥–µ–ª–µ–π')
@click.option('--forward-periods', '-f', default=12, help='–ü–µ—Ä–∏–æ–¥–æ–≤ –≤–ø–µ—Ä–µ–¥ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞')
@click.option('--min-samples', default=500, help='–ú–∏–Ω–∏–º—É–º –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è')
def main(data_dir: str, models_dir: str, forward_periods: int, min_samples: int):
    """–ü—Ä–æ—Å—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ ML –º–æ–¥–µ–ª–µ–π –Ω–∞ scikit-learn"""
    
    print("üöÄ –ü—Ä–æ—Å—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ ML –º–æ–¥–µ–ª–µ–π (–±–µ–∑ XGBoost)")
    print("=" * 50)
    
    try:
        trainer = SimpleMLTrainer(output_dir=models_dir)
        
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        df = trainer.load_data(data_dir)
        
        if len(df) < min_samples:
            print(f"‚ö†Ô∏è  –í–Ω–∏–º–∞–Ω–∏–µ: –¥–∞–Ω–Ω—ã—Ö –º–∞–ª–æ ({len(df)} < {min_samples}), –Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º...")
        
        # 2. –†–∞—Å—á–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        features_df = trainer.calculate_features(df)
        
        # 3. –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–æ–≤
        target_df = trainer.create_targets(features_df, forward_periods)
        
        # 4. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X, y = trainer.prepare_training_data(target_df)
        
        if len(X) < min_samples // 2:
            print(f"‚ùå –ü–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö: {len(X)}")
            return False
        
        # 5. –û–±—É—á–µ–Ω–∏–µ
        scores = trainer.train_models(X, y)
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç
        avg_score = np.mean(list(scores.values()))
        print("=" * 50)
        print(f"üéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"üìä –°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å: {avg_score:.3f}")
        
        if avg_score > 0.4:
            print("‚úÖ –ú–æ–¥–µ–ª–∏ –≥–æ—Ç–æ–≤—ã –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!")
            print("üîÑ –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ —Ç–æ—Ä–≥–æ–≤–æ–≥–æ –±–æ—Ç–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π")
        else:
            print("‚ö†Ô∏è  –ö–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π –Ω–∏–∑–∫–æ–µ, –Ω–æ –æ–Ω–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
        
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {str(e)}")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)